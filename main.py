# åŸºäº Transformer çš„ä¸­è‹±æ–‡ç¿»è¯‘æ¨¡å‹

# å¯¼å…¥å¿…è¦çš„åº“
import gradio as gr
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from sklearn.model_selection import train_test_split
import math

# æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')

# Step 1: æ•°æ®é¢„å¤„ç†
# è¯»å–åŸå§‹æ•°æ®å¹¶æå–è‹±æ–‡å’Œä¸­æ–‡å¥å­

file_path = 'data/cmn.txt'  # è¯·ç¡®ä¿æ•°æ®æ–‡ä»¶ä½äºè¯¥è·¯å¾„ä¸‹

# è¯»å–æ–‡ä»¶å¹¶å¤„ç†æ¯ä¸€è¡Œï¼Œæå–è‹±æ–‡å’Œä¸­æ–‡å¥å­
data = []
with open(file_path, 'r', encoding='utf-8') as file:
    for line in file:
        # æ¯è¡Œæ•°æ®ä½¿ç”¨åˆ¶è¡¨ç¬¦åˆ†å‰²ï¼Œæå–è‹±æ–‡å’Œä¸­æ–‡éƒ¨åˆ†
        parts = line.strip().split('\t')
        if len(parts) >= 2:
            english_sentence = parts[0].strip()
            chinese_sentence = parts[1].strip()
            data.append([english_sentence, chinese_sentence])

# åˆ›å»º DataFrame ä¿å­˜æå–çš„å¥å­
df = pd.DataFrame(data, columns=['English', 'Chinese'])

# å°†å¤„ç†åçš„è‹±æ–‡å’Œä¸­æ–‡å¥å­åˆ†åˆ«ä¿å­˜ä¸ºä¸¤ä¸ªæ–‡ä»¶
df['English'].to_csv('data/english_sentences.txt', index=False, header=False)
df['Chinese'].to_csv('data/chinese_sentences.txt', index=False, header=False)

# æ˜¾ç¤ºå‰äº”è¡Œæ•°æ®
print(df.head())

# Step 2: æ•°æ®åŠ è½½ä¸åˆ†è¯

# å®šä¹‰è‹±æ–‡å’Œä¸­æ–‡çš„åˆ†è¯å™¨
tokenizer_en = get_tokenizer('basic_english')

# ä¸­æ–‡åˆ†è¯å™¨ï¼šå°†æ¯ä¸ªæ±‰å­—ä½œä¸ºä¸€ä¸ª token
def tokenizer_zh(text):
    return list(text)

# æ„å»ºè¯æ±‡è¡¨çš„å‡½æ•°
def build_vocab(sentences, tokenizer):
    """
    æ ¹æ®ç»™å®šçš„å¥å­åˆ—è¡¨å’Œåˆ†è¯å™¨æ„å»ºè¯æ±‡è¡¨ã€‚
    :param sentences: å¥å­åˆ—è¡¨
    :param tokenizer: åˆ†è¯å™¨å‡½æ•°
    :return: è¯æ±‡è¡¨å¯¹è±¡
    """
    def yield_tokens(sentences):
        for sentence in sentences:
            yield tokenizer(sentence)
    vocab = build_vocab_from_iterator(yield_tokens(sentences), specials=['<unk>', '<pad>', '<bos>', '<eos>'])
    vocab.set_default_index(vocab['<unk>'])  # è®¾ç½®é»˜è®¤ç´¢å¼•ä¸º <unk>
    return vocab

# ä»æ–‡ä»¶ä¸­åŠ è½½å¥å­
with open('data/english_sentences.txt', 'r', encoding='utf-8') as f:
    english_sentences = [line.strip() for line in f]

with open('data/chinese_sentences.txt', 'r', encoding='utf-8') as f:
    chinese_sentences = [line.strip() for line in f]

# æ„å»ºè‹±æ–‡å’Œä¸­æ–‡çš„è¯æ±‡è¡¨
en_vocab = build_vocab(english_sentences, tokenizer_en)
zh_vocab = build_vocab(chinese_sentences, tokenizer_zh)

# å®šä¹‰å°†å¥å­è½¬æ¢ä¸ºç´¢å¼•åºåˆ—çš„å‡½æ•°
def process_sentence(sentence, tokenizer, vocab):
    """
    å°†å¥å­è½¬æ¢ä¸ºç´¢å¼•åºåˆ—ï¼Œå¹¶æ·»åŠ  <bos> å’Œ <eos>
    :param sentence: è¾“å…¥å¥å­
    :param tokenizer: åˆ†è¯å™¨å‡½æ•°
    :param vocab: å¯¹åº”çš„è¯æ±‡è¡¨
    :return: ç´¢å¼•åºåˆ—
    """
    tokens = tokenizer(sentence)
    tokens = ['<bos>'] + tokens + ['<eos>']
    indices = [vocab[token] for token in tokens]
    return indices

# å°†æ‰€æœ‰å¥å­è½¬æ¢ä¸ºç´¢å¼•åºåˆ—
en_sequences = [process_sentence(sentence, tokenizer_en, en_vocab) for sentence in english_sentences]
zh_sequences = [process_sentence(sentence, tokenizer_zh, zh_vocab) for sentence in chinese_sentences]

# åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨

class TranslationDataset(Dataset):
    def __init__(self, src_sequences, trg_sequences):
        self.src_sequences = src_sequences
        self.trg_sequences = trg_sequences

    def __len__(self):
        return len(self.src_sequences)

    def __getitem__(self, idx):
        return torch.tensor(self.src_sequences[idx]), torch.tensor(self.trg_sequences[idx])

def collate_fn(batch):
    """
    è‡ªå®šä¹‰çš„ collate_fnï¼Œç”¨äºå°†æ‰¹æ¬¡ä¸­çš„æ ·æœ¬è¿›è¡Œå¡«å……å¯¹é½
    """
    src_batch, trg_batch = [], []
    for src_sample, trg_sample in batch:
        src_batch.append(src_sample)
        trg_batch.append(trg_sample)
    src_batch = pad_sequence(src_batch, padding_value=en_vocab['<pad>'], batch_first=True)
    trg_batch = pad_sequence(trg_batch, padding_value=zh_vocab['<pad>'], batch_first=True)
    return src_batch, trg_batch

# åˆ›å»ºæ•°æ®é›†å¯¹è±¡
dataset = TranslationDataset(en_sequences, zh_sequences)

# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
train_data, val_data = train_test_split(dataset, test_size=0.1)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
batch_size = 32
train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)

# Step 3: Transformer æ¨¡å‹æ„å»º

# å®šä¹‰å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
class ScaledDotProductAttention(nn.Module):
    def __init__(self, d_k):
        super().__init__()
        self.scale = d_k ** -0.5  # ç¼©æ”¾å› å­

    def forward(self, Q, K, V, mask=None):
        """
        :param Q: [batch_size, heads, seq_len, d_k]
        :param K: [batch_size, heads, seq_len, d_k]
        :param V: [batch_size, heads, seq_len, d_v]
        :param mask: [batch_size, 1, 1, seq_len] æˆ– [batch_size, 1, seq_len, seq_len]
        :return: æ³¨æ„åŠ›è¾“å‡ºå’Œæ³¨æ„åŠ›æƒé‡
        """
        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale  # [batch_size, heads, seq_len, seq_len]
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))
        attn = torch.softmax(scores, dim=-1)
        output = torch.matmul(attn, V)  # [batch_size, heads, seq_len, d_v]
        return output, attn

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        self.d_k = d_model // num_heads
        self.num_heads = num_heads

        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.fc = nn.Linear(d_model, d_model)
        
        self.attention = ScaledDotProductAttention(self.d_k)
        self.dropout = nn.Dropout(0.1)

    def forward(self, Q, K, V, mask=None):
        batch_size = Q.size(0)
        
        # ä¿®æ­£ç»´åº¦å¤„ç†é¡ºåº
        Q = self.w_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.w_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.w_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)

        # è®¡ç®—æ³¨æ„åŠ›
        output, attn = self.attention(Q, K, V, mask)
        
        # ä¿®æ­£ç»´åº¦æ‹¼æ¥
        output = output.transpose(1, 2).contiguous().view(
            batch_size, -1, self.num_heads * self.d_k)
        
        output = self.fc(output)
        return output

# å®šä¹‰å‰é¦ˆç¥ç»ç½‘ç»œ
class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.fc1 = nn.Linear(d_model, d_ff)
        self.fc2 = nn.Linear(d_ff, d_model)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.fc2(self.dropout(self.relu(self.fc1(x))))

# å®šä¹‰ä½ç½®ç¼–ç 
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        # åˆ›å»ºä¸€ä¸ª [max_len, d_model] çš„ä½ç½®ç¼–ç çŸ©é˜µ
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1)  # [max_len, 1]
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        # å¥‡æ•°å’Œå¶æ•°ä½ç½®åˆ†åˆ«ä½¿ç”¨ sin å’Œ cos
        pe[:, 0::2] = torch.sin(position * div_term)  # å¶æ•°ä½ç½®
        pe[:, 1::2] = torch.cos(position * div_term)  # å¥‡æ•°ä½ç½®
        pe = pe.unsqueeze(0)  # å¢åŠ  batch ç»´åº¦
        self.register_buffer('pe', pe)

    def forward(self, x):
        # x: [batch_size, seq_len, d_model]
        x = x + self.pe[:, :x.size(1)].to(x.device)
        return x

# å®šä¹‰ç¼–ç å™¨å±‚
class EncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, num_heads)
        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask=None):
        # è‡ªæ³¨æ„åŠ›å­å±‚
        attn_output = self.self_attn(x, x, x, mask)
        x = x + self.dropout(attn_output)
        x = self.norm1(x)
        # å‰é¦ˆç¥ç»ç½‘ç»œå­å±‚
        ffn_output = self.ffn(x)
        x = x + self.dropout(ffn_output)
        x = self.norm2(x)
        return x

# å®šä¹‰è§£ç å™¨å±‚
class DecoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, num_heads)
        self.cross_attn = MultiHeadAttention(d_model, num_heads)
        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, enc_output, src_mask=None, trg_mask=None):
        # æ©ç å¤šå¤´è‡ªæ³¨æ„åŠ›å­å±‚
        self_attn_output = self.self_attn(x, x, x, trg_mask)
        x = x + self.dropout(self_attn_output)
        x = self.norm1(x)
        # ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›å­å±‚
        cross_attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)
        x = x + self.dropout(cross_attn_output)
        x = self.norm2(x)
        # å‰é¦ˆç¥ç»ç½‘ç»œå­å±‚
        ffn_output = self.ffn(x)
        x = x + self.dropout(ffn_output)
        x = self.norm3(x)
        return x

# å®šä¹‰ç¼–ç å™¨
class Encoder(nn.Module):
    def __init__(self, input_dim, d_model, num_heads, d_ff, num_layers, dropout):
        super().__init__()
        self.d_model = d_model
        self.embedding = nn.Embedding(input_dim, d_model)
        self.pos_encoder = PositionalEncoding(d_model)
        self.layers = nn.ModuleList([
            EncoderLayer(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])
        self.dropout = nn.Dropout(dropout)

    def forward(self, src, src_mask=None):
        # src: [batch_size, src_len]
        x = self.embedding(src) * math.sqrt(self.d_model)
        x = self.pos_encoder(x)
        x = self.dropout(x)
        for layer in self.layers:
            x = layer(x, src_mask)
        return x

# å®šä¹‰è§£ç å™¨
class Decoder(nn.Module):
    def __init__(self, output_dim, d_model, num_heads, d_ff, num_layers, dropout):
        super().__init__()
        self.d_model = d_model
        self.embedding = nn.Embedding(output_dim, d_model)
        self.pos_encoder = PositionalEncoding(d_model)
        self.layers = nn.ModuleList([
            DecoderLayer(d_model, num_heads, d_ff, dropout)
            for _ in range(num_layers)
        ])
        self.dropout = nn.Dropout(dropout)
        self.fc_out = nn.Linear(d_model, output_dim)

    def forward(self, trg, enc_output, src_mask=None, trg_mask=None):
        # trg: [batch_size, trg_len]
        x = self.embedding(trg) * math.sqrt(self.d_model)
        x = self.pos_encoder(x)
        x = self.dropout(x)
        for layer in self.layers:
            x = layer(x, enc_output, src_mask, trg_mask)
        output = self.fc_out(x)
        return output

# å®šä¹‰ Transformer æ¨¡å‹
class Transformer(nn.Module):
    def __init__(self, encoder, decoder):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder

    def make_src_mask(self, src):
        # ç”Ÿæˆæºåºåˆ—çš„æ©ç ï¼Œå±è”½å¡«å……ä½ç½®
        src_mask = (src != en_vocab['<pad>']).unsqueeze(1).unsqueeze(2)
        return src_mask  # [batch_size, 1, 1, src_len]

    def make_trg_mask(self, trg):
        # ç”Ÿæˆç›®æ ‡åºåˆ—çš„æ©ç ï¼ŒåŒ…å«å¡«å……ä½ç½®å’Œæœªæ¥ä¿¡æ¯
        trg_pad_mask = (trg != zh_vocab['<pad>']).unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, trg_len]
        trg_len = trg.size(1)
        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=trg.device)).bool()  # [trg_len, trg_len]
        trg_mask = trg_pad_mask & trg_sub_mask  # [batch_size, 1, trg_len, trg_len]
        return trg_mask

    def forward(self, src, trg):
        src_mask = self.make_src_mask(src)
        trg_mask = self.make_trg_mask(trg)
        enc_output = self.encoder(src, src_mask)
        output = self.decoder(trg, enc_output, src_mask, trg_mask)
        return output

# åˆå§‹åŒ–æ¨¡å‹å‚æ•°
input_dim = len(en_vocab)
output_dim = len(zh_vocab)
d_model = 512
num_heads = 8
d_ff = 2048
num_layers = 3
dropout = 0.1

# å®ä¾‹åŒ–ç¼–ç å™¨ã€è§£ç å™¨å’Œ Transformer æ¨¡å‹
encoder = Encoder(input_dim, d_model, num_heads, d_ff, num_layers, dropout)
decoder = Decoder(output_dim, d_model, num_heads, d_ff, num_layers, dropout)
model = Transformer(encoder, decoder).to(device)

# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss(ignore_index=zh_vocab['<pad>'])
optimizer = optim.Adam(model.parameters(), lr=0.0001)

# Step 4: æ¨¡å‹è®­ç»ƒä¸éªŒè¯

# å®šä¹‰è®­ç»ƒå‡½æ•°
def train(model, dataloader, optimizer, criterion):
    model.train()
    epoch_loss = 0
    for src, trg in dataloader:
        src = src.to(device)
        trg = trg.to(device)
        optimizer.zero_grad()
        output = model(src, trg[:, :-1])  # è¾“å…¥ä¸åŒ…æ‹¬æœ€åä¸€ä¸ªè¯
        output_dim = output.shape[-1]
        output = output.contiguous().view(-1, output_dim)
        trg = trg[:, 1:].contiguous().view(-1)  # ç›®æ ‡ä¸åŒ…æ‹¬ç¬¬ä¸€ä¸ªè¯
        loss = criterion(output, trg)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    return epoch_loss / len(dataloader)

# å®šä¹‰éªŒè¯å‡½æ•°
def evaluate(model, dataloader, criterion):
    model.eval()
    epoch_loss = 0
    with torch.no_grad():
        for src, trg in dataloader:
            src = src.to(device)
            trg = trg.to(device)
            output = model(src, trg[:, :-1])
            output_dim = output.shape[-1]
            output = output.contiguous().view(-1, output_dim)
            trg = trg[:, 1:].contiguous().view(-1)
            loss = criterion(output, trg)
            epoch_loss += loss.item()
    return epoch_loss / len(dataloader)

# å¼€å§‹è®­ç»ƒ
n_epochs = 10

for epoch in range(n_epochs):
    train_loss = train(model, train_dataloader, optimizer, criterion)
    val_loss = evaluate(model, val_dataloader, criterion)
    print(f'Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss:.3f}, Val Loss: {val_loss:.3f}')

# Step 5: æµ‹è¯•ä¸æ¨ç†

# å®šä¹‰ç¿»è¯‘å‡½æ•°
def translate_sentence(sentence, model, en_vocab, zh_vocab, tokenizer_en, max_len=50):
    """
    ç¿»è¯‘è‹±æ–‡å¥å­ä¸ºä¸­æ–‡
    :param sentence: è‹±æ–‡å¥å­ï¼ˆå­—ç¬¦ä¸²ï¼‰
    :param model: è®­ç»ƒå¥½çš„ Transformer æ¨¡å‹
    :param en_vocab: è‹±æ–‡è¯æ±‡è¡¨
    :param zh_vocab: ä¸­æ–‡è¯æ±‡è¡¨
    :param tokenizer_en: è‹±æ–‡åˆ†è¯å™¨
    :param max_len: æœ€å¤§ç¿»è¯‘é•¿åº¦
    :return: ä¸­æ–‡ç¿»è¯‘ï¼ˆå­—ç¬¦ä¸²ï¼‰
    """
    model.eval()
    tokens = tokenizer_en(sentence)
    tokens = ['<bos>'] + tokens + ['<eos>']
    src_indices = [en_vocab[token] for token in tokens]
    src_tensor = torch.LongTensor(src_indices).unsqueeze(0).to(device)  # [1, src_len]
    src_mask = model.make_src_mask(src_tensor)
    with torch.no_grad():
        enc_output = model.encoder(src_tensor, src_mask)
    trg_indices = [zh_vocab['<bos>']]
    for i in range(max_len):
        trg_tensor = torch.LongTensor(trg_indices).unsqueeze(0).to(device)  # [1, trg_len]
        trg_mask = model.make_trg_mask(trg_tensor)
        with torch.no_grad():
            output = model.decoder(trg_tensor, enc_output, src_mask, trg_mask)
        pred_token = output.argmax(-1)[:, -1].item()
        trg_indices.append(pred_token)
        if pred_token == zh_vocab['<eos>']:
            break
    trg_tokens = [zh_vocab.lookup_token(idx) for idx in trg_indices]
    return ''.join(trg_tokens[1:-1])  # å»é™¤ <bos> å’Œ <eos>

# å®šä¹‰ Gradio äº¤äº’å‡½æ•°ï¼ˆæ·»åŠ å¼‚å¸¸å¤„ç†å’Œè¾“å…¥å‡€åŒ–ï¼‰
def gradio_translate(text: str) -> str:
    try:
        # ç©ºè¾“å…¥å¤„ç†
        if not text.strip():
            return "è¯·è¾“å…¥æœ‰æ•ˆçš„è‹±æ–‡å¥å­"
        
        # æ‰§è¡Œç¿»è¯‘
        translated = translate_sentence(
            sentence=text,
            model=model,
            en_vocab=en_vocab,
            zh_vocab=zh_vocab,
            tokenizer_en=tokenizer_en
        )
        
        # å¤„ç†ç‰¹æ®Šæ ‡è®°å’Œç©ºç™½
        translated = translated.replace('<unk>', '').strip()
        return translated if translated else "[ç©ºç¿»è¯‘]"
    
    except KeyError as e:
        return f"è¯æ±‡è¡¨ç¼ºå¤±å…³é”®æ ‡è®°ï¼š{str(e)}"
    except RuntimeError as e:
        return f"æ¨¡å‹æ¨ç†é”™è¯¯ï¼š{str(e)}"
    except Exception as e:
        return f"æœªçŸ¥é”™è¯¯ï¼š{str(e)}"

# æ„å»º Gradio ç•Œé¢
def create_interface():
    with gr.Blocks(title="Transformer ç¿»è¯‘å™¨", theme=gr.themes.Soft()) as interface:
        gr.Markdown("# ğŸ€„â‡„ğŸ…° åŸºäº Transformer çš„è‹±ä¸­ç¿»è¯‘å™¨")
        
        with gr.Row():
            with gr.Column():
                input_box = gr.Textbox(
                    label="è¾“å…¥è‹±æ–‡",
                    placeholder="åœ¨æ­¤è¾“å…¥è¦ç¿»è¯‘çš„è‹±æ–‡...",
                    lines=3,
                    max_lines=5
                )
                examples = gr.Examples(
                    examples=[
                        ["Hello, how are you?"],
                        ["What is your name?"],
                        ["The weather is nice today."]
                    ],
                    inputs=[input_box]
                )
                
            output_box = gr.Textbox(
                label="è¾“å‡ºä¸­æ–‡",
                placeholder="ç¿»è¯‘ç»“æœå°†æ˜¾ç¤ºåœ¨æ­¤å¤„...",
                lines=3,
                show_copy_button=True
            )
        
        with gr.Row():
            submit_btn = gr.Button("ç¿»è¯‘", variant="primary")
            clear_btn = gr.Button("æ¸…ç©º")
        
        # äº‹ä»¶ç»‘å®š
        submit_btn.click(
            fn=gradio_translate,
            inputs=input_box,
            outputs=output_box
        )
        clear_btn.click(
            lambda: ("", ""),
            outputs=[input_box, output_box]
        )
    
    return interface

# ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    # æ‰§è¡Œæ¨¡å‹è®­ç»ƒ
    print("ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
    for epoch in range(n_epochs):
        train_loss = train(model, train_dataloader, optimizer, criterion)
        val_loss = evaluate(model, val_dataloader, criterion)
        print(f'Epoch {epoch+1}/{n_epochs}, è®­ç»ƒæŸå¤±: {train_loss:.3f}, éªŒè¯æŸå¤±: {val_loss:.3f}')
    
    # å¯åŠ¨äº¤äº’ç•Œé¢
    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼Œå¯åŠ¨ç¿»è¯‘ç•Œé¢...")
    interface = create_interface()
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        show_error=True,
        share=False
    )